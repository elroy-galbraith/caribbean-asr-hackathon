{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4d0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4806312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "AUDIO_DIR = Path(\"../../data/raw/Audio\")  # Relative to this script location\n",
    "TRAIN_CSV = Path(\"../../data/raw/Train.csv\")\n",
    "EVAL_SIZE = 0.15  # 15% for eval\n",
    "HOLDOUT_SIZE = 0.10  # 10% for holdout (from remaining data)\n",
    "RANDOM_STATE = 42\n",
    "OUTPUT_DIR = Path(\"../../data/splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06344a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_duration(audio_path):\n",
    "    \"\"\"Extract duration from audio file\"\"\"\n",
    "    try:\n",
    "        duration = librosa.get_duration(path=audio_path)\n",
    "        return duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_duration_bins(durations, n_bins=4):\n",
    "    \"\"\"Create duration bins based on quantiles\"\"\"\n",
    "    if n_bins == 3:\n",
    "        bins = [0, 4, 8, np.inf]\n",
    "        labels = ['short', 'medium', 'long']\n",
    "    else:  # 4 bins based on quartiles\n",
    "        q1, q2, q3 = np.percentile(durations, [25, 50, 75])\n",
    "        bins = [0, q1, q2, q3, np.inf]\n",
    "        labels = ['short', 'medium_short', 'medium_long', 'long']\n",
    "    \n",
    "    return bins, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32bc48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Total samples: 19856\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "    \n",
    "print(f\"Total samples: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9946e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting audio durations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19856/19856 [00:12<00:00, 1629.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract durations for all audio files\n",
    "print(\"\\nExtracting audio durations...\")\n",
    "durations = []\n",
    "valid_indices = []\n",
    "    \n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    audio_path = AUDIO_DIR / f\"{row['ID']}.wav\"  # Adjust extension if needed\n",
    "    duration = extract_audio_duration(audio_path)\n",
    "        \n",
    "    if duration is not None:\n",
    "        durations.append(duration)\n",
    "        valid_indices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2696b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid samples: 19856\n",
      "Duration statistics:\n",
      "count    19856.000000\n",
      "mean         6.049944\n",
      "std          3.386629\n",
      "min          2.000000\n",
      "25%          3.442000\n",
      "50%          5.301000\n",
      "75%          7.643000\n",
      "max         29.694000\n",
      "Name: duration, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filter to valid samples only\n",
    "train_df = train_df.iloc[valid_indices].copy()\n",
    "train_df['duration'] = durations\n",
    "    \n",
    "print(f\"\\nValid samples: {len(train_df)}\")\n",
    "print(f\"Duration statistics:\")\n",
    "print(train_df['duration'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c7c0c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating duration bins...\n",
      "\n",
      "Duration bin distribution:\n",
      "duration_bin\n",
      "short           4969\n",
      "medium_short    4965\n",
      "medium_long     4963\n",
      "long            4959\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create duration bins for stratification\n",
    "print(\"\\nCreating duration bins...\")\n",
    "bins, labels = create_duration_bins(train_df['duration'].values, n_bins=4)\n",
    "train_df['duration_bin'] = pd.cut(train_df['duration'], bins=bins, labels=labels)\n",
    "    \n",
    "print(\"\\nDuration bin distribution:\")\n",
    "print(train_df['duration_bin'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f60b0dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Splitting off eval set (15.0%)...\n"
     ]
    }
   ],
   "source": [
    "# First split: separate out eval set\n",
    "print(f\"\\nStep 1: Splitting off eval set ({EVAL_SIZE*100}%)...\")\n",
    "train_temp_indices, eval_indices = train_test_split(\n",
    "    train_df.index,\n",
    "    test_size=EVAL_SIZE,\n",
    "    stratify=train_df['duration_bin'],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "    \n",
    "train_temp = train_df.loc[train_temp_indices].copy()\n",
    "eval_set = train_df.loc[eval_indices].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c9f92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Splitting train and holdout (10.0% of total)...\n"
     ]
    }
   ],
   "source": [
    "# Second split: separate train and holdout from remaining data\n",
    "# Calculate holdout size relative to the temporary training set\n",
    "holdout_size_adjusted = HOLDOUT_SIZE / (1 - EVAL_SIZE)\n",
    "    \n",
    "print(f\"Step 2: Splitting train and holdout ({HOLDOUT_SIZE*100}% of total)...\")\n",
    "train_indices, holdout_indices = train_test_split(\n",
    "    train_temp.index,\n",
    "    test_size=holdout_size_adjusted,\n",
    "    stratify=train_temp['duration_bin'],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "    \n",
    "train_set = train_df.loc[train_indices].copy()\n",
    "holdout_set = train_df.loc[holdout_indices].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9f8ba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPLIT VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Dataset sizes:\n",
      "  Train:    14891 (75.0%)\n",
      "  Eval:      2979 (15.0%)\n",
      "  Holdout:   1986 (10.0%)\n",
      "  Total:    19856\n",
      "\n",
      "Duration bin distribution (proportions):\n",
      "\n",
      "Train:\n",
      "duration_bin\n",
      "short           0.250285\n",
      "medium_short    0.250017\n",
      "medium_long     0.249950\n",
      "long            0.249748\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Eval:\n",
      "duration_bin\n",
      "short           0.250084\n",
      "medium_short    0.250084\n",
      "medium_long     0.250084\n",
      "long            0.249748\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Holdout:\n",
      "duration_bin\n",
      "short           0.250252\n",
      "medium_short    0.250252\n",
      "medium_long     0.249748\n",
      "long            0.249748\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Duration statistics:\n",
      "\n",
      "Train:\n",
      "count    14891.000000\n",
      "mean         6.048614\n",
      "std          3.381003\n",
      "min          2.000000\n",
      "25%          3.442000\n",
      "50%          5.301000\n",
      "75%          7.643000\n",
      "max         29.468000\n",
      "Name: duration, dtype: float64\n",
      "\n",
      "Eval:\n",
      "count    2979.000000\n",
      "mean        6.051557\n",
      "std         3.364252\n",
      "min         2.000000\n",
      "25%         3.442500\n",
      "50%         5.289000\n",
      "75%         7.642500\n",
      "max        29.209000\n",
      "Name: duration, dtype: float64\n",
      "\n",
      "Holdout:\n",
      "count    1986.000000\n",
      "mean        6.057497\n",
      "std         3.463082\n",
      "min         2.001000\n",
      "25%         3.441500\n",
      "50%         5.293000\n",
      "75%         7.638500\n",
      "max        29.694000\n",
      "Name: duration, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Validation: Check distribution preservation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPLIT VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "    \n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train:    {len(train_set):5d} ({len(train_set)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"  Eval:     {len(eval_set):5d} ({len(eval_set)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"  Holdout:  {len(holdout_set):5d} ({len(holdout_set)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"  Total:    {len(train_df):5d}\")\n",
    "    \n",
    "print(\"\\nDuration bin distribution (proportions):\")\n",
    "print(\"\\nTrain:\")\n",
    "print(train_set['duration_bin'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nEval:\")\n",
    "print(eval_set['duration_bin'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nHoldout:\")\n",
    "print(holdout_set['duration_bin'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "print(\"\\nDuration statistics:\")\n",
    "for name, dataset in [(\"Train\", train_set), (\"Eval\", eval_set), (\"Holdout\", holdout_set)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(dataset['duration'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c579e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING SPLITS\n",
      "================================================================================\n",
      "\n",
      "Files saved to ..\\..\\data\\splits/:\n",
      "  Core splits (for modeling):\n",
      "    - train.csv\n",
      "    - eval.csv\n",
      "    - holdout.csv\n",
      "\n",
      "  With metadata (for analysis):\n",
      "    - train_with_metadata.csv\n",
      "    - eval_with_metadata.csv\n",
      "    - holdout_with_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Save splits\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING SPLITS\")\n",
    "print(\"=\"*80)\n",
    "    \n",
    "# Save clean versions (just ID and Transcription)\n",
    "train_set[['ID', 'Transcription']].to_csv(OUTPUT_DIR / 'train.csv', index=False)\n",
    "eval_set[['ID', 'Transcription']].to_csv(OUTPUT_DIR / 'eval.csv', index=False)\n",
    "holdout_set[['ID', 'Transcription']].to_csv(OUTPUT_DIR / 'holdout.csv', index=False)\n",
    "    \n",
    "# Save versions with metadata\n",
    "train_set.to_csv(OUTPUT_DIR / 'train_with_metadata.csv', index=False)\n",
    "eval_set.to_csv(OUTPUT_DIR / 'eval_with_metadata.csv', index=False)\n",
    "holdout_set.to_csv(OUTPUT_DIR / 'holdout_with_metadata.csv', index=False)\n",
    "    \n",
    "print(f\"\\nFiles saved to {OUTPUT_DIR}/:\")\n",
    "print(\"  Core splits (for modeling):\")\n",
    "print(\"    - train.csv\")\n",
    "print(\"    - eval.csv\")\n",
    "print(\"    - holdout.csv\")\n",
    "print(\"\\n  With metadata (for analysis):\")\n",
    "print(\"    - train_with_metadata.csv\")\n",
    "print(\"    - eval_with_metadata.csv\")\n",
    "print(\"    - holdout_with_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6acfc96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Summary:\n",
      "    - split_summary.csv\n",
      "\n",
      "✓ Split creation complete!\n"
     ]
    }
   ],
   "source": [
    "# Save split info summary\n",
    "summary = {\n",
    "    'total_samples': len(train_df),\n",
    "    'train_samples': len(train_set),\n",
    "    'eval_samples': len(eval_set),\n",
    "    'holdout_samples': len(holdout_set),\n",
    "    'train_pct': len(train_set)/len(train_df)*100,\n",
    "    'eval_pct': len(eval_set)/len(train_df)*100,\n",
    "    'holdout_pct': len(holdout_set)/len(train_df)*100,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'stratification': 'duration_bin (4 bins based on quartiles)'\n",
    "}\n",
    "    \n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(OUTPUT_DIR / 'split_summary.csv', index=False)\n",
    "print(\"\\n  Summary:\")\n",
    "print(\"    - split_summary.csv\")\n",
    "    \n",
    "print(\"\\n✓ Split creation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
