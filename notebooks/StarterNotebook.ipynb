{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 0. Install libraries\n",
        "# ==========================================\n",
        "!pip install -q transformers librosa soundfile\n",
        "\n",
        "print(\"Libraries installed.\")"
      ],
      "metadata": {
        "id": "F0XkihDSwXje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. Mount Google Drive (force remount to be safe)\n",
        "# ==========================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "Iz9L2HuBwYMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. Unzip Audio.zip (robust)\n",
        "# ==========================================\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "drive_zip_path = '/content/drive/MyDrive/Hackathon/Audio.zip'\n",
        "local_zip_path = '/content/Audio.zip'\n",
        "audio_dir = 'audio_files'\n",
        "\n",
        "# Check that the file exists on Drive\n",
        "if not os.path.exists(drive_zip_path):\n",
        "    raise FileNotFoundError(f\"Could not find Audio.zip at: {drive_zip_path}\")\n",
        "\n",
        "print(f\"Found zip on Drive: {drive_zip_path}\")\n",
        "\n",
        "# Copy zip locally to avoid Drive I/O issues\n",
        "shutil.copy(drive_zip_path, local_zip_path)\n",
        "print(f\"Copied zip to local path: {local_zip_path}\")\n",
        "\n",
        "# Create extraction directory\n",
        "os.makedirs(audio_dir, exist_ok=True)\n",
        "\n",
        "# Extract from local copy\n",
        "with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(audio_dir)\n",
        "\n",
        "print(f\"Audio files extracted to: {audio_dir}\")"
      ],
      "metadata": {
        "id": "yd84nMOqwYSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. Load Test.csv\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "\n",
        "test_csv_path = '/content/drive/MyDrive/Hackathon/Test.csv'\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "print(\"Test DataFrame head:\")\n",
        "print(test_df.head())\n",
        "print(\"Total test files:\", len(test_df))"
      ],
      "metadata": {
        "id": "Dl91-aVEwYWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. Load pretrained ASR model (no fine-tuning)\n",
        "# ==========================================\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "\n",
        "MODEL_CHECKPOINT = \"facebook/wav2vec2-base-960h\"\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(MODEL_CHECKPOINT)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = \"cuda\" if use_cuda else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Using GPU:\", use_cuda)\n",
        "print(\"Model loaded on:\", device)\n",
        "if not use_cuda:\n",
        "    print(\"⚠️ WARNING: Running on CPU will be very slow for ~8,000 files. \"\n",
        "          \"In Colab, go to Runtime → Change runtime type → set Hardware accelerator to GPU.\")"
      ],
      "metadata": {
        "id": "dFx2AazVwhBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. Transcribe test audio\n",
        "# ==========================================\n",
        "import librosa\n",
        "import time\n",
        "\n",
        "AUDIO_SR = 16000       # target sample rate\n",
        "MAX_SECONDS = 30\n",
        "\n",
        "predictions = []\n",
        "\n",
        "start_time = time.time()\n",
        "n_files = len(test_df)\n",
        "\n",
        "for idx, row in test_df.iterrows():\n",
        "    audio_id = row[\"ID\"]\n",
        "    audio_path = os.path.join(audio_dir, f\"{audio_id}.wav\")\n",
        "\n",
        "    if not os.path.exists(audio_path):\n",
        "        print(f\"Warning: missing audio file: {audio_path}\")\n",
        "        predictions.append(\"\")\n",
        "        continue\n",
        "\n",
        "    # Progress log every 100 files\n",
        "    if idx % 100 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Processing file {idx}/{n_files}  |  Elapsed: {elapsed:.1f}s\")\n",
        "\n",
        "    # Load audio\n",
        "    speech_array, sr = librosa.load(audio_path, sr=AUDIO_SR)\n",
        "\n",
        "    # Truncate for safety/speed\n",
        "    max_len = int(MAX_SECONDS * AUDIO_SR)\n",
        "    if len(speech_array) > max_len:\n",
        "        speech_array = speech_array[:max_len]\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = processor(\n",
        "        speech_array,\n",
        "        sampling_rate=AUDIO_SR,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(\n",
        "        predicted_ids,\n",
        "        skip_special_tokens=True\n",
        "    )[0]\n",
        "\n",
        "    predictions.append(transcription.strip())\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"Number of predictions:\", len(predictions))\n",
        "print(f\"Total inference time: {total_time/60:.1f} minutes\")\n",
        "\n",
        "\n",
        "print(\"\\nSample predictions:\")\n",
        "for i in range(min(5, len(predictions))):\n",
        "    print(f\"ID: {test_df['ID'].iloc[i]}, Pred: '{predictions[i]}'\")"
      ],
      "metadata": {
        "id": "YtJrnrFbwhGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. Create SampleSubmission.csv\n",
        "# ==========================================\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": test_df[\"ID\"],\n",
        "    \"Transcription\": predictions\n",
        "})\n",
        "\n",
        "submission_path = \"SampleSubmission.csv\"\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"Saved submission file to: {submission_path}\")\n",
        "print(submission_df.head())"
      ],
      "metadata": {
        "id": "z-DYNX56whLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}